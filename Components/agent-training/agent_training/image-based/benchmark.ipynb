{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Montvieux Ltd\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import PIL.Image\n",
    "from IPython.display import display,clear_output,HTML\n",
    "from IPython.display import Image as DisplayImage\n",
    "import base64\n",
    "import json\n",
    "from io import StringIO\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "from plark_game import classes\n",
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os, sys\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "from gym_plark.envs import plark_env,plark_env_guided_reward,plark_env_top_left\n",
    "\n",
    "# %matplotlib inline\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import datetime\n",
    "basepath = '/data/agents/models'\n",
    "\n",
    "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "import helper \n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = plark_env_guided_reward.PlarkEnvGuidedReward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)\n",
    "\n",
    "\n",
    "## Define the player type we are training.\n",
    "modelplayer = \"pelican\"\n",
    "## Define the type of RL algorithm you are using.\n",
    "modeltype = \"dqn\"\n",
    "## Specify the date and time for this training.\n",
    "basicdate = str(datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "\n",
    "## Used for generating the json header file which holds details regarding the model.\n",
    "## This will be used when playing the game from the GUI.\n",
    "\n",
    "\n",
    "# Instantiate the env\n",
    "env = plark_env_guided_reward.PlarkEnvGuidedReward()\n",
    "\n",
    "model = DQN('CnnPolicy', env)\n",
    "#model = A2C('CnnPolicy', env)\n",
    "model.learn(500)\n",
    "\n",
    "retrain_iter = []\n",
    "retrain_values = []\n",
    "\n",
    "print(\"****** STARTING EVALUATION *******\")\n",
    "mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=5, deterministic=False, render=False, callback=None, reward_threshold=None, return_episode_rewards=False)\n",
    "retrain_iter.append(str(0))\n",
    "retrain_values.append(mean_reward)\n",
    "\n",
    "def save():\n",
    "    print(str(retrain_iter))\n",
    "    print(str(retrain_values))\n",
    "    \n",
    "    plt.figure(figsize=(9, 3))\n",
    "    plt.subplot(131)\n",
    "    plt.bar(retrain_iter, retrain_values)\n",
    "    plt.subplot(132)\n",
    "    plt.scatter(retrain_iter, retrain_values)\n",
    "    plt.subplot(133)\n",
    "    plt.plot(retrain_iter, retrain_values)\n",
    "    plt.suptitle('Retraining Progress')\n",
    "    ##plt.show()\n",
    "    model_path,model_dir, modellabel = save_model_with_env_settings(basepath,model,modeltype,env,basicdate)\n",
    "    fig_path = os.path.join(model_dir, 'Training_Progress.png')\n",
    "    plt.savefig(fig_path)\n",
    "    print('Model saved to ', model_path)\n",
    "\n",
    "\n",
    "def retrain(mean_reward, target_reward, count):\n",
    "    while mean_reward < target_reward:\n",
    "        count = count + 1\n",
    "        retrain_iter.append(str(count))\n",
    "        \n",
    "        model.learn(50)\n",
    "        \n",
    "        mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=1, deterministic=False, render=False, callback=None, reward_threshold=None, return_episode_rewards=False)\n",
    "        retrain_values.append(mean_reward)\n",
    "        \n",
    "        if mean_reward > target_reward:\n",
    "           save()\n",
    "           break\n",
    "        if mean_reward < target_reward:\n",
    "            retrain(mean_reward, target_reward, count)\n",
    "    \n",
    "    print(\"Model Training Reached Target Level\")\n",
    "\n",
    "retrain(mean_reward, 10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
